#!/usr/bin/bash
#SBATCH -p vertex
#SBATCH -J tp7
#SBATCH --nodes 1
#SBATCH --ntasks-per-node 128
#SBATCH --cpus-per-task 1
#SBATCH -o output%j.txt
#SBATCH -e error%j.txt
#SBATCH -t 1:00:00

export OMP_PROC_BIND=true
export OMP_PLACES=cores
export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK

export IOSS_PROPERTIES="COMPOSE_RESULTS=on:MINIMIZE_OPEN_FILES=on:MAXIMUM_NAME_LENGTH=64"
export EXODUS_NETCDF4=1

cd $SLURM_SUBMIT_DIR

echo "
SLURM_JOB_ID
$SLURM_JOB_ID

SLURM_JOB_NUM_NODES
$SLURM_JOB_NUM_NODES

SLURM_NTASKS
$SLURM_NTASKS

SLURM_CPUS_PER_TASK
$SLURM_CPUS_PER_TASK

SLURM_JOB_NODELIST
$SLURM_JOB_NODELIST

"

module use /projects/mp_common/spack_env/v0.4/modulefiles
module use /projects/mp_common/spack_env/v0.4/spack/share/spack/modules/linux-rhel8-zen2/
module load gcc/gcc-11.2.0
module load openmpi/4.1.1

# Special case for single MPI rank per node
if [ ${SLURM_NTASKS_PER_NODE} -eq 1 ]
then
  MAP_STRING="ppr:1:node:pe=${SLURM_CPUS_PER_TASK}"
else
  # Compute MPI mapping -- assuming a node has 2 sockets
  NUM_SOCKETS=2
  let "TASKS_PER_SOCKET = ${SLURM_NTASKS_PER_NODE} / ${NUM_SOCKETS}"
  MAP_STRING="ppr:${TASKS_PER_SOCKET}:socket:pe=${SLURM_CPUS_PER_TASK}"
fi

EXE=<PATH_TO_VERTEXCFD_EXECUTABLE>
INPUTFILE=<INPUT_FILENAME>
mpirun --map-by ${MAP_STRING} $EXE --i=$INPUTFILE

